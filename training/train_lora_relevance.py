# training/train_lora_relevance.py
"""
Train a relevance classifier-style LoRA model that answers YES/NO.

Input: data/relevance_dataset.jsonl (generated by create_relevance_dataset.py)
Each record should be: {"id":"...","text":"...","label":"YES"/"NO"}

This script prepares instruction-style prompts and fine-tunes a causal LM
to generate either "YES" or "NO" given the prompt.

Usage (example):
  export BASE_MODEL="tiiuae/falcon-7b-instruct"
  python training/train_lora_relevance.py \
    --train_jsonl data/relevance_dataset.jsonl \
    --base_model $BASE_MODEL \
    --output_dir models/lora_relevance \
    --epochs 2 \
    --batch_size 2 \
    --qlora

Notes:
  - If you pass --qlora, this will attempt 4-bit QLoRA training (requires bitsandbytes)
  - This script uses label masking: prompt tokens are masked with -100 in labels so loss is computed only on target tokens.
  - The generated target for positives is "YES" and for negatives "NO".
Requirements:
  pip install datasets transformers accelerate peft sentence-transformers bitsandbytes  # bitsandbytes only for QLoRA
"""
import argparse
import json
import os
from datasets import load_dataset, Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import Trainer, TrainingArguments
from transformers import DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
import torch

PROMPT_TEMPLATE = (
    "Instruction: Decide whether the following text is relevant for UPSC Civil Services Mains examination.\n\n"
    "Text:\n{TEXT}\n\nAnswer with YES or NO (only the word YES or NO)."
)

# Target tokens are just "YES" or "NO". We will append a newline after them as termination.
def build_example(text: str, label: str):
    prompt = PROMPT_TEMPLATE.format(TEXT=text.strip())
    # model input is prompt + " " + target
    target = label.strip().upper()  # "YES" or "NO"
    full = prompt + " " + target + "\n"
    # we'll also return prompt length in tokens later during tokenization
    return {"text": full, "prompt": prompt, "target": target}

def tokenize_examples(examples, tokenizer, max_length):
    # examples: list of dicts with 'text' (full) and 'prompt' (prompt only)
    enc = tokenizer(examples["text"], truncation=True, max_length=max_length, padding=False)
    # compute prompt token length to mask prompt tokens in labels
    with tokenizer.as_target_tokenizer():
        prompt_enc = tokenizer(examples["prompt"], truncation=True, max_length=max_length, padding=False)
    # build labels: copy input_ids then mask prompt tokens with -100
    input_ids = enc["input_ids"]
    labels = []
    for i, ids in enumerate(input_ids):
        # determine prompt token length by encoding prompt separately
        prompt_ids = prompt_enc["input_ids"][i]
        prompt_len = len(prompt_ids)
        lab = ids.copy()
        for j in range(min(prompt_len, len(lab))):
            lab[j] = -100
        labels.append(lab)
    result = {"input_ids": input_ids, "attention_mask": enc["attention_mask"], "labels": labels}
    return result

def main(args):
    # load raw dataset (jsonl)
    ds = load_dataset("json", data_files=args.train_jsonl)["train"]
    print(f"[INFO] Raw dataset examples: {len(ds)}")

    # convert to prompt+target style with "YES" / "NO"
    def to_prompt(rec):
        lbl = rec.get("label", "").strip().upper()
        if lbl not in ("YES", "NO"):
            # if label is anything else, interpret "relevant" as YES
            lbl = "YES" if rec.get("label", "").lower().startswith("y") else "NO"
        ex = build_example(rec.get("text", ""), lbl)
        return {"text": ex["text"], "prompt": ex["prompt"], "target": ex["target"]}
    ds = ds.map(to_prompt)

    # train/val split
    ds = ds.train_test_split(test_size=args.val_split, seed=42)
    train_ds = ds["train"]
    val_ds = ds["test"]

    print("[INFO] Preparing tokenizer and model:", args.base_model)
    tokenizer = AutoTokenizer.from_pretrained(args.base_model, use_fast=True)
    # ensure tokenizer has pad token
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({"pad_token": "[PAD]"})
    # tokenization map function
    def tok_map(batch):
        return tokenize_examples(batch, tokenizer, args.max_length)
    # map tokenization
    train_tok = train_ds.map(tok_map, batched=True, remove_columns=list(train_ds.column_names))
    val_tok = val_ds.map(tok_map, batched=True, remove_columns=list(val_ds.column_names))

    # load model
    model = AutoModelForCausalLM.from_pretrained(
        args.base_model,
        device_map="auto",
        load_in_4bit=args.qlora,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        trust_remote_code=True
    )

    if args.qlora:
        model = prepare_model_for_kbit_training(model)

    # LoRA config
    lora_config = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        target_modules=args.target_modules.split(",") if args.target_modules else None,
        lora_dropout=args.lora_dropout,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    training_args = TrainingArguments(
        output_dir=args.output_dir,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.grad_accum,
        num_train_epochs=args.epochs,
        learning_rate=args.learning_rate,
        fp16=torch.cuda.is_available(),
        evaluation_strategy="epoch" if args.do_eval else "no",
        save_strategy="epoch",
        logging_steps=50,
        remove_unused_columns=False,
        push_to_hub=False
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_tok,
        eval_dataset=val_tok if args.do_eval else None,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )

    trainer.train()
    trainer.save_model(args.output_dir)
    tokenizer.save_pretrained(args.output_dir)
    print("[DONE] Model saved to", args.output_dir)

if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--train_jsonl", default="data/relevance_dataset.jsonl")
    p.add_argument("--base_model", required=True, help="HuggingFace model id or path (causal LM)")
    p.add_argument("--output_dir", default="models/lora_relevance")
    p.add_argument("--epochs", type=int, default=1)
    p.add_argument("--batch_size", type=int, default=2)
    p.add_argument("--grad_accum", type=int, default=4)
    p.add_argument("--learning_rate", type=float, default=2e-4)
    p.add_argument("--val_split", type=float, default=0.05)
    p.add_argument("--max_length", type=int, default=256)
    p.add_argument("--qlora", action="store_true", help="Use 4-bit QLoRA (requires bitsandbytes)")
    p.add_argument("--lora_r", type=int, default=8)
    p.add_argument("--lora_alpha", type=int, default=16)
    p.add_argument("--lora_dropout", type=float, default=0.05)
    p.add_argument("--target_modules", type=str, default="q_proj,v_proj", help="comma-separated target module names for LoRA")
    p.add_argument("--do_eval", action="store_true")
    args = p.parse_args()
    main(args)
